## 4.3 主题模型的贝叶斯深度学习

在本节中，我们回顾了将BDL用于主题模型的一些示例。这些模型结合了PGM（它自然地结合了变量之间的概率关系）和NN（它有效地学习深度表示）的优点，从而显著提高了性能。

### 4.3.1 关系堆栈去噪自动编码器作为主题模型

***问题声明和符号***。假设我们有一组项目（文章或电影）$X_c$，$X^T_{c,j*}$表示项目$j$的内容（属性）。此外，我们用$I_K$表示$K$维单位矩阵，用$S=[s_1,s_2,...,s_J]$表示关系潜在矩阵，$s_j$表示项$j$的关系属性。

从SDAE的角度来看，J-by-B矩阵$X_c$代表SDAE的干净输入，相同大小的噪声损坏矩阵用$X_0$表示。此外，我们用$X_l$表示SDAE的第$l$层的输出，即$J-by-K_l$矩阵。$X_l$的第$j$行用$X_{l.j*}$表示，$W_l$和$b_l$是层$l$的权重矩阵和偏置向量，$W_{l,*n}$表示$W_l$的列$n$，$L$是层数。作为速记，我们将所有层中的权重矩阵和偏差的收集称为$W^+$。请注意，$L/2$层SDAE对应$L$层网络。

***模型配方***。在这里，我们将使用之前介绍的贝叶斯SDAE作为关系堆栈去噪自动编码器（RSDAE）模型的构建块。

如【64】所述，RSDAE被表述为一个新的概率模型，它可以无缝地集成分层表示学习和可用的关系信息。这样，模型就可以同时从内容信息和项目之间的关系中学习特征表示。RSDAE的图形模型如图7所示，生成过程如下：

- 从矩阵变量正态分布中绘制关系潜在矩阵S[21]：
  $$
  S\sim\mathcal{N}_{K,J}(0,I_K\otimes(\lambda_l\mathcal{L}_a)^{-1})
  $$

- 对于SDAE的第$l$层，其中$l=1,2,...,\frac{L}{2}-1$ ，

  - 对于权重矩阵$W_l$的每一列$n$，取出$W_{l,j*}\sim\mathcal{N}(0,\lambda_w^{-1}I_{K_l})$。
  - 取出偏执向量$b_l\sim\mathcal{N}(0,\lambda_w^{-1}I_{K_l})$
  - 对于$X_l$的每一行$j$，取出$X_{l,j*}\sim\mathcal{N}(\sigma(X_{l-1,j*}W_l+b_l,\lambda_s^{-1}I_{K_l}))$

- 对于SDAE网络的$\frac{L}{2}$层，从两个高斯（PoG）的乘积中绘制项目j的表示向量【16】：
  $$
  X_{\frac{L}{2},j*}\sim PoG(\sigma(X_{\frac{L}{2}-1,j*}W_l+b_l),s_j^T,\lambda_s^{-1}I_K,\lambda_r^{-1}I_K)
  $$

- 对于SDAE网络的每一层$l$，$l=\frac{L}{2}+1,\frac{L}{2}+2,...,L$，
  - 对于权重矩阵$W_l$的每一列$n$，取出$W_{l,*n}\sim\mathcal{N}(0,\lambda_w^{-1}I_{K_l})$
  - 取出偏执向量$b_l\sim\mathcal{N}(0,\lambda_w^{-1}I_{K_l})$
  - 对于$X_l$的每一行$j$，取出$X_{l,j*}\sim\mathcal{N}(\sigma(X_{l-1,j*}W_l+b_l),\lambda_s^{-1}I{K_l})$
- 对于每个item $j$，取出$X_{l,j*}\sim\mathcal{N}(X_{L,j*},\lambda_n^{-1}I_B)$

这里$K=K_{\frac{L}{2}}$是每个项的学习表示向量的维数，$S$表示$K\times J$关系潜在矩阵，其中列$j$是项j的关系潜在向量$s_j$。请注意，方程（10）中的$\mathcal{N}_{K,J}(0,I_K\otimes(\lambda_l\mathcal{L}_a)^{-1})$是【21】中定义的矩阵变量正态分布，其中运算符$\otimes$表示两个矩阵【21】的克罗内克积，$tr(·)$表示矩阵的迹线，$\mathcal{L}_a$是包含关系信息的拉普拉斯矩阵。$\mathcal{L}_a=D-A$，其中$D$是对角矩阵，其对角元素$D_{ii}=\sum_jA_{ij}$和$A$是表示关系信息的邻接矩阵，二进制条目指示项目之间的链接（或关系）。$A_{jj'}$表示项目$j$和项目$j'$之间存在链接，否则$A_{jj'=0}$。$PoG(\sigma(X_{\frac{F}{2}-1,j*}W_l+b_l),s_j^T,\lambda_s^{-1}I_K,\lambda_r^{-1}I_K)$表示高斯$\mathcal{N}(\sigma(X_{\frac{L}{2}-1,j*}W_l+b_l),\lambda_s^{-1}I_K)$和高斯$\mathcal{N}(s_j^T,\lambda_r^{-1}I_K)$的乘积，$\mathcal{N}(s_j^T,\lambda_r^{-1}I_K)$也是高斯【16】。

根据上面的生成过程，最大化后验概率相当于最大化$\{X_l\},X_c,S,\{W_l\}$和$\{b_l\}$的联合对数似然性，给定$\lambda_w,\lambda_l,\lambda_r$和$\lambda_n$：
$$
\begin{align*}
\mathcal{L}=&-\frac{\lambda_l}{2}tr(S\mathcal{L}_aS^T)-\frac{\lambda_r}{2}\sum_j||(s_j^T-X_{\frac{L}{2},j*})||^2_2\\
&-\frac{\lambda_w}{2}\sum\limits_l(||W_l||^2_F+||b_l||^2_2)\\
&-\frac{\lambda_n}{2}\sum\limits_j||X_{L,j*}-X_{c,j*}||^2_2\\
&-\frac{\lambda_s}{2}\sum\limits_l\sum\limits_j||\sigma(X_{l-1,j*}W_l+b_l)-X_{l,j*}||^2_2
\end{align*}
$$
其中，$X_{l,j*}=\sigma(X_{l-1,j*}W_l+b_l)$。请注意，第一项$-\frac{\lambda_l}{2}tr(S\mathcal{L}_aS^T)$对应于方程（12）中矩阵变量分布中的$log\ p(S)$。通过简单的操作，我们有$tr(S\mathcal{L}_aS^T)=\sum^K_{k=1}S^T_{k*}\mathcal{L}_aS_{k*}$，其中$S_{k*}$表示$S$的第$k$行。正如我们所看到的，如果item $j$和item $j'$连接，最大化$-\frac{\lambda_l}{2}tr(S^T\mathcal{L}_aS)$相当于使$s_j$更接近$s_{j'}$（即$A_{jj'}=1$）。

在RSDAE中，感知变量$\Omega_p=\{\{X_l\},X_c,\{W_l\},\{b_l\}\}$，中枢变量$\Omega_h=\{S\}$，和任务变量$\Omega_t=\{A\}$。

学习关系表示和主题。[64]提供了一种EM风格的MAP估计算法。在这里，我们回顾了以下一些关键步骤。

就关系潜矩阵$S$而言，我们首先修复外$S$的所有行，除了第$k$行$S_{k*}$之外，然后更新$S_{k*}$。具体来说，我们取$\mathcal{L}$相对于$S_{k*}$的梯度，将其设置为0，并得到以下线性系统：
$$
(\lambda_l\mathcal{L}_a+\lambda_rI_J)S_{k*}=\lambda^T_{\frac{L}{2},*k}
$$
一种幼稚的方法是通过设置$S_{k*}=\lambda_r(\lambda_l\mathcal{L}_a+\lambda_rI_J)^{-1}X^T_{\frac{L}{2},*k}$来求解线性系统。不幸的是，一个更新的复杂性是$O(J^3)$。与【38】类似，最陡下降方法【55】用于迭代更新$S_{k*}$：
$$
\begin{align*}
S_{k*}(t&+1)\leftarrow S_{k*}(t)+\delta(t)r(t)\\
&r(t)\leftarrow\lambda_rX^T_{\frac{L}{2},*k}-(\lambda_l\mathcal{L}_a+\lambda_rI_J)S_{k*}(t)\\
&\delta(t)\leftarrow \frac{r(t)^Tr(t)}{r(t)^T(\lambda_l\mathcal{L}_a+\lambda_rI_J)r(t)}
\end{align*}
$$
如【38】所述，使用最陡下降方法显著降低了从$O(J^3)$到$O(J)$的每次迭代中的计算成本。

给定$S$，我们可以使用反向传播算法学习每层的$W_l$和$b_l$。通过交替更新$S$、$W_l$和$b_l$，可以找到$\mathcal{L}$的局部最优。此外，包括动量项等技术可能有助于避免陷入局部最优。每次迭代的计算复杂度为$O(JBK_1+KJ)$。与CDL类似，没有引入显著的开销。

表3显示了当学习到的表示用于标签推荐时，数据集电影镜头图中不同方法的召回（每个项目有300个推荐标签）。正如我们所看到的，RSDAE的性能明显优于SDAE，这意味着来自任务特定组件的关系信息对性能提升至关重要。有关更多详细信息，请参阅【64】。

### 4.3.2 基于Sigmoid信念网络的深度泊松因子分析

支持非负整数的泊松分布被称为建模计数的自然选择。因此，最好将其用作主题模型【8】的构建块。基于这一动机，【75】提出了一个模型，称为泊松因子分析（PFA），用于通过泊松分布进行潜在非负矩阵分解。

泊松因子分析。PFA假设一个离散的N-by-P矩阵$X$，其中包含$N$个文档的字数，词汇量大小为P [17], [75]。简言之，PFA可以使用以下公式来描述：
$$
X\sim Pois((H)\Phi)
$$
其中$\Phi$（大小K-by-P，其中$K$是主题的数量）表示因子分析中的因子加载矩阵，第$k$行$\phi_k$编码主题$k$中每个单词的重要性。N乘K矩阵$Q$是因子评分矩阵，第$n$行$\theta_n$包含文档$n$的主题比例。N乘K矩阵$H$是一个潜在的二进制矩阵，第$n$行$h_n$定义了与文档$n$关联的主题集。

不同的先验对应不同的模型。例如，$\phi_k$和$\theta_n$上的Dirichlet先验与全一矩阵$H$将恢复LDA [8]，而$h_n$上的β-Bernoulli先验导致负二项式聚焦主题模型（NB-FTM）模型在【74】。在【17】中，将基于sigmoid信念网络(SBN) [43]（一种具有二进制隐藏单元的MLP变体）的深度结构先验强加于$h_n$，以形成用于主题建模的深度PFA模型。

深度泊松因子分析。在深度PFA模型【17】中，生成过程可总结如下：
