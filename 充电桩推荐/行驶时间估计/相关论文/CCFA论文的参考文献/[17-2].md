## 4.2 推荐系统的贝叶斯深度学习

尽管深度学习在自然语言处理和计算机视觉上取得了成功的应用，但很少有人尝试为CF开发深度学习模型。[54]中的作者使用受限玻尔兹曼机器而不是传统的矩阵分解公式来执行CF,并且[19]通过结合用户和项目相关性来扩展这项工作。虽然这些方法涉及深度学习和CF，但它们实际上属于基于CF的方法，因为它们不像CTR [60]那样包含内容信息，这对于准确推荐至关重要。[52]中的作者在深度网络的最后一个权重层中使用低阶矩阵分解，以显著减少模型参数的数量并加快训练速度，但它是用于分类而不是推荐任务。在音乐推荐方面，[45]、【68】直接使用传统CNN或深度信仰网络（DBN）来帮助内容信息的表征学习，但它们模型的深度学习组件是确定性的，而不对噪声建模，因此它们的鲁棒性较低。这些模型主要通过松耦合的方法来实现性能提升，而不利用内容信息和评级之间的相互作用。此外，CNN直接链接到评级矩阵，这意味着当评级稀疏时，模型将由于严重的过拟合而表现不佳。

### 4.2.1 协同深度学习

为了应对上述挑战，在[67]中引入了一种称为**协同深度学习（CDL）**的分层贝叶斯模型，作为一种新的RS紧耦合方法。基于SDAE的贝叶斯公式，CDL将内容信息的深度表征学习与评分（反馈）矩阵的协同过滤紧密结合，允许两者之间的双向交互。实验表明，CDL显著优于现有技术。

在下面的文本中，我们将从介绍CDL演示过程中使用的符号开始。之后，我们将回顾CDL的设计和学习。

***符号和问题表述***。与【60】中的工作类似，CDL中考虑的推荐任务将隐式反馈【30】作为训练和测试数据。$J$项（文章或电影）的整个集合由J-by-B矩阵$X_c$表示，其中第$j$行是单词包向量$X_{c.j*}$，基于大小为$B$的词汇表的$j$项。对于$I$用户，我们定义了一个I-by-J二进制评级矩阵$R=[R_{ij}]_{I\times J}$。例如，在数据集中cieulike-a[60]、[62]、【67】如果用户$i$在其个人库中有文章$j$，则$R_{ij}=1$，否则$R_{ij}=0$。给定$R$中的部分评级和内容信息$X_c$，问题是预测$R$中的其他评级。请注意，尽管CDL目前形式侧重于电影推荐（电影情节被视为内容信息）和文章推荐，如【60】在本节中，它足够通用，可以处理其他推荐任务（例如，标记推荐）。

矩阵$X_c$扮演SDAE的干净输入的角色，而噪声损坏的矩阵，也是J-by-B矩阵，用$X_0$表示。SDAE的第l层的输出用$X_l$表示，$X_l$是$J$乘$K_l$矩阵，其中$K_l$是第l层中的单元数。与$X_c$类似，$X_l$的第$j$行由$X_{l.j*}$表示。$W_l$和$b_l$分别是层$l$的权重矩阵和偏置向量，$W_l,*n$表示$W_l$的列$n$,L是层数。为了方便起见，我们使用$W^+$来表示所有层权重矩阵和偏差的集合。请注意，$L/2$层SDAE对应$L$层网络。

广义贝叶斯的SDAE。根据第2.2节对SDAE的介绍，如果我们假设干净的输入$X_c$和损坏的输入$X_0$都被观察到，类似于[4]，[5]，[12]，[41]，我们可以定义以下广义贝叶斯SDAE的生成过程。

- 对于SDAE网络的每一层$l$:

  - 对于权重矩阵$W_l$的每一列$n$，取出：$W_{l,*n} \sim \mathcal{N}(0,\lambda_w^{-1}I_{K_l})$

  - 取出偏置向量$b_l\sim \mathcal{N}(0,\lambda_w^{-1}I_{K_l})$

  - 对于$X_l$的每一行$j$,取出：$W_{l,j*} \sim \mathcal{N}(\sigma(X_{l-1},j*),\lambda_s^{-1}I_{K_l})$

- 对于每个item $j$，取出一个干净的输入：$X_{c,j*} \sim \mathcal{N}(X_{L,j*},\lambda_n^{-1}I_{B})$

请注意，如果$\lambda_s$趋向无穷大，方程（7）中的高斯分布将变成$\sigma(X_{l-1,j*}W_l+b_l)$为中心的Dirac delta分布[58]，其中$\sigma(\cdot)$是sigmoid函数。该模型将退化为SDAE的贝叶斯公式。这就是为什么我们称它为广义SDAE。

请注意，网络的前$L/2$层作为一个编码器，最后$L/2$层作为解码器。后验概率的最大化等同于考虑到权重衰减后重建误差的最小化。

协同深度学习。以贝叶斯SDAE为组件，CDL的生成过程定义如下：

- 生成广义贝叶斯SDAE的变量。

- 对于每个item $j$,
  - 绘制潜在item偏移向量$\epsilon_j \sim \mathcal{N}(0,\lambda_v^{-1}I_K)$，然后设置潜在item向量：$v_j=\epsilon_j+X^T_{\frac{L}{2},j*}$。

- 为每个用户$i$绘制一个潜在的用户向量：$u_i\sim\mathcal{N}(0,\lambda_u^{-1}I_K)$

4. 为每个用户-item对$(i,j)$取出一个评级$R_{ij}$，即$R_{ij}\sim\mathcal{N}(u_i^Tv_j,C_{ij}^{-1})$。

这里$\lambda_w,\lambda_n,\lambda_u,\lambda_s$和$\lambda_v$是超参数，$C_{ij}$是类似于CTR[60]的置信度参数（如果$R_{ij}=1$，则$C_{ij}=a$，否则$C_{ij}=b$）。请注意，中间层$X_{L/2}$作为评级和内容信息之间的桥梁。这个中间层，连同潜在的偏移量$\epsilon_j$，是使CDL同时学习有效的特征表示和捕捉项目（和用户）之间的相似性和（隐性）关系的关键。与广义的SDAE类似，为了提高计算效率，我们也可以将$\lambda_s$取为无穷大。

当$\lambda_s$接近正无穷大时，CDL的图形模型如图4所示，为了简化符号，我们分别用$x_0,x_{L/2}$和$x_C$来替代$X^T_{0,j*},X^T_{\frac{L}{2},j*}$和$X^T_{c,j*}$。

### 图4

请注意，根据第4.1节的定义，这里的感知变量$\Omega_p={\{\{W_l\},\{b_l\},\{X_l\},X_c\}}$，中枢变量$\Omega_h=\{V\}$，以及任务变量$\Omega_t=\{U,R\}$，其中$V=(v_j)^J_{j=1}$，$U=(u_i)^I_{i=1}$。

***学习***。基于上述的CDL模型，所有的参数可以被视为随机变量，因此可以采用完全的贝叶斯方法，如马尔科夫链蒙特卡洛或变异的近似方法[32]可以被应用。然而，这种处理方法通常会产生很高的计算成本。因此，CDL使用EM风格的算法来获得MAP估计，如[60]。

与CTR [60]一样，最大后验概率相当于最大$U,V,\{X_l\},\{X_c\},\{W_l\},\{b_l\}$和$R$的联合对数似然，给定$\lambda_u,\lambda_v,\lambda_w,\lambda_s$和$\lambda_n$：
$$
\begin{align*}
\mathcal{L} &= - \frac{\lambda_u}{2}\sum\limits_i||u_i||^2_2-\frac{\lambda_w}{2}\sum\limits_l(||W_l||^2_F+||b_l||^2_2)
 \\ 
&-\frac{\lambda_v}{2}\sum\limits_j||v_j-X^T_{\frac{L}{2},j*}||^2_2-\frac{\lambda_n}{2}\sum\limits_j||X_{L,j*}-X_{c,j*}||^2_2 \\
&-\frac{\lambda_s}{2}\sum\limits_l\sum\limits_j||\sigma(X_{l-1,j*}+b_l)-X_{l,j*}||^2_2 \\
&-\sum\limits_{i,j}\frac{C_{ij}}{2}(R_{ij}-u^T_iv_j)^2.
\end{align*}
$$
如果$\lambda_s$趋向无穷大，则似然变为：
$$
\begin{align*}
\mathcal{L} &= - \frac{\lambda_u}{2}\sum\limits_i||u_i||^2_2-\frac{\lambda_w}{2}\sum\limits_l(||W_l||^2_F+||b_l||^2_2)
 \\ 
&-\frac{\lambda_v}{2}\sum\limits_j||v_j-f_e(X_{0,j*},W^+)^T||^2_2 \\
&-\frac{\lambda_n}{2}\sum\limits_j||f_r(X_{0,j*},W^+)-X_{c,j*}||^2_2 \\
&-\sum\limits_{i,j}\frac{C_{ij}}{2}(R_{ij}-u^T_iv_j)^2.
\end{align*}
$$
其中，编码器函数$f_e(·,W^+)$将项目$j$的损坏内容向量$X_{0,j*}$作为输入，并计算项目的编码，函数$f_r(·,W^+)$也将$X_{0,j*}$作为输入，计算编码，然后重建项$j$的内容向量。例如，如果层数$L=6$，则$f_e(X_{0,j*},W^+)$是第三层的输出，而$f_r(X_{0,j*},W^+)$是第六层的输出。

从优化的角度来看，上述目标函数（8）中的第三项等效于使用潜在项向量$v_j$作为目标的多层每感知器，而第四项等效于SDAE，使重建误差最小化。从神经网络（NN）的角度来看，当$\lambda_s$接近正无穷大时，图4（左）中CDL的概率图形模型的训练将退化为同时训练两个与公共输入层（损坏的输入）重叠在一起的神经网络)但不同的输出层，如图5所示。请注意，由于评级矩阵的参与，第二个网络比典型的神经网络复杂得多。

### 图5

当$\lambda_n/\lambda_v$的比例接近正无穷大时，它将退化为一个两步模型，其中使用SDAE学习的潜在表示直接放入CTR中。感知组件和任务特定组件之间的交互是单向的（从感知组件到任务特定组件），这意味着感知组件不会受到任务特定组件的影响。另一个极端发生在$\lambda_n/\lambda_v$变为零时，SDAE的解码器基本上消失了。图4的右侧是当$\lambda_n/\lambda_v$变为零时退化CDL的图形模型。正如实验中所证明的那样，在两种极端情况下，预测性能都将受到很大的影响[67]。这验证了（1）来自任务特定组件的信息可以改善感知组件，（2）相互增强效应对BDL至关重要。

对于$u_i$和$v_j$，使用类似于[30]、【60】的块坐标下降。给定当前$W^+$，我们计算$\mathcal{L}$相对于$u_i$和$v_j$的梯度，然后将它们设置为零，从而产生以下更新规则：
$$
\begin{align*}
&u_i\leftarrow(VC_iV^T+\lambda_uI_K)^{-1}VC_iR_i \\
&v_j\leftarrow(UC_iU^T+\lambda_vI_K)^{-1})(UC_jR_j+\lambda_vf_e(X_{0,j*},W^+)^T)
\end{align*}
$$
其中，$U=(u_i)^I_{i=1}$，$V=(v_j)^J_{j=1}$，$C_i=diag(C_{i1},...,C_{i,J})$为对角矩阵，$R_i=(R_{i1},...,R_{iJ})^T$是包含用户$i$的所有评级的列向量，$C_{ij}$反映了【30】中讨论的由$a$和$b$控制的置信度。$C_j$和$R_j$对item $j$的定义类似。

给定$U$和$V$，我们可以使用反向传播（BP）学习算法学习每层的权重$W_l$和偏置$b_l$。相对于$W_l$和$b_l$的似然梯度如下：
$$
\begin{align*}
\Delta_{W_l}\mathcal{L} &= -\lambda_w W_l \\
&-\lambda_v\sum\limits_j\Delta_{W_l}f_e(X_{0,j*},W^+)^T(f_e(X_{0,j*},W^+)^T-v_j) \\
&-\lambda_n\sum\limits_j\Delta_{W_l}f_r(X_{0,j*},W^+)(f_r(X_{0,j*},W^+)-X_{c,j*}) \\


\Delta_{b_l}\mathcal{L} &= -\lambda_w b_l \\
&-\lambda_v\sum\limits_j\Delta_{b_l}f_e(X_{0,j*},W^+)^T(f_e(X_{0,j*},W^+)^T-v_j) \\
&-\lambda_n\sum\limits_j\Delta_{b_l}f_r(X_{0,j*},W^+)(f_r(X_{0,j*},W^+)-X_{c,j*}) \\
\end{align*}
$$
通过交替更新$U$、$V$、$W_l$和$b_l$，我们可以找到$\cal L$的局部最优。可以应用几种常用的技术，如使用动量项来缓解局部最优问题。请注意，精心设计的BDL模型（根据i.i.d.要求，并具有第4.1节所述的适当方差模型）可以最大限度地减少无缝组合感知组件和任务特定组件的开销。在CDL中，感知分量的计算复杂度（每次迭代）为$O(JBK_1)$，任务特定分量的计算复杂度为$O(K^2N_R+K^3)$，其中$N_R$是评级矩阵中的非零条目数，$K=K_{\frac{L}{2}}$。整个模型的计算复杂度（每次迭代）$O(JBK_1+K^2N_R+K^3)$[67]。没有引入重大开销。

***预测***。设$D$为观察到的测试数据。与【60】类似，CDL使用$u_i$、$W^+$和$\epsilon j$的点估计来计算预测评级：
$$
E[R_{ij}|D]\approx E[u_i|D]^T(E[f_e(X_{0,j*},W^+)^T|D]+E[\epsilon_j|D])
$$
其中，$E[·]$表示期望操作。换句话说，我们将预测评级近似为：
$$
R_{ij}^*\approx(u_j^*)^T(f_e(X_{0,j*},W^{+^*})^T+\epsilon_j^*)=(u_i^*)^Tv_j^*
$$
请注意，对于训练数据中没有评级的任何新item $j$，其偏移量$\epsilon_j^*$将为0。表2显示了对数据集 citeulike-a中不同方法的300个推荐项目的召回。有关更多详细信息，请参阅【67】。在下文中，我们从不同的角度提供了CDL的几个扩展。

### 4.2.2 协同贝叶斯深度学习

除了MAP估计之外，【67】还提出了一种基于采样的CDL贝叶斯处理算法。该算法原来是众所周知的反向传播学习算法的贝叶斯和广义版本。我们列出了关键条件密度，如下所示：

对于$W^+$。我们将$W_{l,*n}$和$b_l^{(n)}$的级联表示为$W^+_{l,*n}$。类似地，$X_{l,j*}$和$1$的级联被表示为$X^+_{l,j*}$。忽略$i$的下标。然后：
$$
\begin{align*}
&p(W^+_{l,*n}|X_{l-1,j*},X_{l,j*},\lambda_s) \\
&\propto \mathcal{N}(W^+_{l,*n}|0,\lambda^{-1}_wI) \mathcal{N}
(X_{l,*n}|\sigma(X_{l,*n}^+), \lambda_s^{-1} I).
\end{align*}
$$
对于$X_{l,j*}(l\neq L/2)$。类似地，我们将$W_l$和$b_l$的级联表示为$W_l^+$，有：
$$
p(X_{l,j*}|W^+_l,W^+_{l+1},X_{l-1,j*},X_{l+1,j*}\lambda_s) \\
\propto \mathcal{N}(X_{l,j*}|\sigma(X^+_{l-1,j*}W^+_l),\lambda_s^{-1}I). \\
 \mathcal{N}(X_{l+1,j*}|\sigma(X^+_{l,j*}W^+_{l+1}),\lambda_s^{-1}I).
$$
注意，对于最后一层$(l=L)$，第二个高斯将是$\mathcal{N}(X_{c,j*}|X_{l,j*},\lambda_s^{-1}I)$。

对于$X_{l,j*}(l=L/2)$。类似的，我们有：
$$
p(X_{l,j*}|W^+_l,W^+_{l+1},X_{l-1,j*},X_{l+1,j*}\lambda_s\lambda_v\lambda_j) \\
\propto \mathcal{N}(X_{l,j*}|\sigma(X^+_{l-1,j*}W^+_l),\lambda_s^{-1}I). \\
 \mathcal{N}(X_{l+1,j*}|\sigma(X^+_{l,j*}W^+_{l+1}),\lambda_s^{-1}I)\mathcal{N}(v_j|X_{l,j*},\lambda_v^{-1}I).
$$
对于$v_j$。后验概率$p(v_j|X_{L/2},j*,R_{*,j},C_{*j},\lambda_v,U) \propto \mathcal{N}(v_j|X^T_{L/2},j*,\lambda_v^{-1}I)\prod\limits_i\mathcal{N}(R_{ij}|u_i^Tv_j,C_{ij}^{-1}).$

对于$u_i$。后验概率$p(u_i|R_{i*},V,\lambda_u,C_{i*})\propto\mathcal{N}(u_i|0,\lambda_u^{-1}I)\prod\limits_j(R_{ij}|u^T_iv_j|C_{ij}^{-1}).$

有趣的是，如果$\lambda_s$进入无穷大，并且使用自适应拒绝大都会采样（包括使用目标函数的梯度来近似建议分布），则$W^+$的采样结果是BP的贝叶斯广义版本。具体来说，如图6所示，在某一点（左边的红色虚线）获得损失函数的梯度后，下一个样本将在该线下的区域中绘制，这相当于BP的概率版本。如果样本位于损失函数曲线上方，则将添加一条新的切线（右侧的黑色虚线），以更好地近似与关节对数似然相对应的分布。之后，将从两条线下的区域抽取样本。在采样过程中，除了使用梯度（MAP）搜索局部最优外，该算法还考虑了方差。这就是为什么它被称为贝叶斯广义反向传播。

### 图6

### 4.2.3 边缘化协同深度学习

在SDAE中，损坏的输入经过编码和解码以恢复干净的输入。通常，不同的训练时代使用不同的损坏版本作为输入。因此，一般来说，SDAE需要经历足够的培训时代，以看到足够多的损坏版本的输入。边缘化SDAE (mSDAE) [13]试图通过边缘化损坏的输入并直接获得封闭形式的解决方案来避免这种情况。从这个意义上讲，mSDAE比SDAE更有效。
