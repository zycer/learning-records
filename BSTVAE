在本节中我们提出了一种名为贝叶斯时空变分自编码器模型BSTVAE，这是一个融合了贝叶斯理论、时空卷积网络（STGCN）以及变分自编码器VAE的深度学习模型。我们的方法旨在处理不确定性和融合时空信息，以提高预测性能。在本节中，我们将详细介绍贝叶斯自编码器的构建及其运作原理。

为了捕获交通数据的潜在表示，我们采用变分自编码器(VAE)架构。变分自编码器是一种生成性概率模型，它使用编码器和解码器两个神经网络来学习数据的潜在表示。通过最大化边界似然的下界来学习潜在变量的近似后验分布。

假设输入数据为 x，潜在变量为 z。我们希望学习数据的概率分布 P(x)，其中 P(z) 表示潜在变量的先验分布，在本文中，我们采用标准正态分布作为先验分布，P(x|z) 表示给定潜在变量 z 时 x 的条件分布。由于直接计算后验分布 P(z|x) 是困难的，VAE 引入了一个变分分布 Q(z|x) 来近似后验分布。我们的目标是最大化观测数据的边缘对数似然的下界（ELBO），即：

ELBO = E_q[log P(x|z)] - D_KL(Q(z|x) || P(z))

其中 E_q[log P(x|z)] 表示重构误差，即在潜在空间的采样 z 生成原始数据 x 的概率的期望，用于度量编码器和解码器的重构能力；D_KL(Q(z|x) || P(z)) 表示编码器输出的分布与先验分布之间的 Kullback-Leibler 散度，KL散度衡量了从编码器学习到的潜在分布 Q(z|x) 与先验分布 P(z) 的差异，用于度量潜在空间中的表示是否符合我们的先验知识。最优化 ELBO 会使得 VAE 学到的表示在重构和符合先验知识之间达到平衡。在我们的模型中，我们将编码器和解码器分别用神经网络实现，编码器网络将输入数据 x 映射到概率分布参数 μ(x) 和 logvar(x)，解码器网络将潜在空间的采样 z 映射回数据空间 x'，以最小化重构误差。具体而言，编码器和解码器可以表示为：

μ(x), logvar(x) = Encoder(x)

x' = Decoder(z)

为了训练 VAE，我们可以使用随机梯度下降（SGD）或其他优化算法最大化 ELBO。在训练过程中，我们可以采用重参数化技巧（reparameterization trick）进行梯度传播，即通过引入一个独立于参数的噪声变量 ε，将潜在变量 z 表示为：

z = μ(x) + exp(0.5 * logvar(x)) * ε

其中 ε 服从标准正态分布。

具体来说，BSTVAE模型包括以下组成部分：

1. 时空图卷积网络（STGCN）：这部分包括了三层FeastConv，分别用于从输入特征中提取时空相关信息，每层STGCN后借一个Relu激活函数以增加非线性。
2. 贝叶斯图卷积编码器，用于将输入特征映射到潜在空间的均值和方差；
3. 重参数化操作，用于引入不确定性；
4. 贝叶斯图卷积解码器，用于从潜在空间表示重建输入特征；
5. 边缘行驶时间预测部分，用于预测关键指标。

接下来，我们逐一介绍BSTVAE的组成部分。

1. 时空图卷积层（STGCN）

时空图卷积网络（STGCN）用于在图结构中捕捉交通网络中的时空依赖关系，包括时序关系以及局部邻居信息。STGCN包括两层FeaStConv层，用于学习输入特征的高级表示。FeaStConv 是一种用于图结构数据的卷积操作，其基本思想是将每个节点的特征与其邻居的特征结合起来。FeaStConv 的灵感来源于一种名为图神经网络（GNN）的神经网络结构，其目的是在图上学习节点表征[1]。FeaStConv 的计算过程如下所示：

FeaStConv层的计算可以表示为：

通过训练 VAE，我们可以学习到潜在空间中的数据表示，从而提高交通网络行驶时间预测的准确性和鲁棒性。此外，VAE 的生成性能力还可以用于生成新的交通数据样本，从而在一定程度上丰富数据集。
